{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先前自己弄了一個簡單的class-Data_Loader來做訓練過程中取資料的物件，但人生苦短，何苦要重複造輪，一點都不pythonic，TensorFlow 2.0提供了`tf.data`這個便利的工具，來看看怎麼使用吧。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "指定硬體資源，相關可[參考](https://hackmd.io/@shaoeChen/ryWIV4vkL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices(device_type='GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name=u'/physical_device:GPU:0', device_type=u'GPU'),\n",
       " PhysicalDevice(name=u'/physical_device:GPU:1', device_type=u'GPU')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.config.experimental.set_visible_devices(devices=gpus[1], device_type='GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.experimental.set_memory_growth(device=gpus[1], enable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "原本的class就不要用了，讓它隨風去，一樣取得MNIST資料集並做標準化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = np.expand_dims(x_train / 255., -1)\n",
    "x_test = np.expand_dims(x_test / 255., -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 28, 28, 1), (10000, 28, 28, 1))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, x_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "將資料集與標籤做為參數提供給`tf.data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = tf.data.Dataset.from_tensor_slices((x_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset shapes: ((28, 28, 1), ()), types: (tf.float64, tf.uint8)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = datasets.shuffle(buffer_size=128, seed=10).batch(128).repeat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "利用標準的keras Sequential來建置模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.InputLayer(input_shape=(28, 28, 1)),\n",
    "    tf.keras.layers.Conv2D(filters=6, kernel_size=(5, 5), padding='valid', activation='tanh'),\n",
    "    tf.keras.layers.MaxPool2D(pool_size=(2, 2)),\n",
    "    tf.keras.layers.Conv2D(filters=16, kernel_size=(5, 5), padding='valid', activation='tanh'),\n",
    "    tf.keras.layers.MaxPool2D(pool_size=(2, 2)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(120, activation='tanh'),\n",
    "    tf.keras.layers.Dense(84, activation='tanh'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax'),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "確認模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 24, 24, 6)         156       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 12, 12, 6)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 8, 8, 16)          2416      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 4, 4, 16)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 120)               30840     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 84)                10164     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                850       \n",
      "=================================================================\n",
      "Total params: 44,426\n",
      "Trainable params: 44,426\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "編譯模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "    loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "訓練模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 468 steps\n",
      "Epoch 1/5\n",
      "468/468 [==============================] - 5s 10ms/step - loss: 1.0567 - accuracy: 0.7367\n",
      "Epoch 2/5\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 0.3488 - accuracy: 0.9068\n",
      "Epoch 3/5\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 0.2288 - accuracy: 0.9363\n",
      "Epoch 4/5\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 0.1722 - accuracy: 0.9514\n",
      "Epoch 5/5\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 0.1405 - accuracy: 0.9603\n",
      "CPU times: user 18.9 s, sys: 2.1 s, total: 21 s\n",
      "Wall time: 15.5 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f5b0d9bdc10>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model.fit(datasets,\n",
    "          epochs=5, \n",
    "          steps_per_epoch=int(len(x_train)/128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以發現到，`model.fit`的時候我們沒有提供batch_size，因為這在datasets指定了，也沒有指定x_train與y_train，而是直接提供datasets這個物件，很方便吧。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tf.data.Dataset`非常便利，簡單瞭解一下它有那些功能"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先是`tf.data.Dataset.prefetch`，它是一個預取機制，當GPU在訓練的時候，CPU可能正在喝杯飲料等GPU忙完叫它，等GPU訓練好一批資料之後再跟CPU說，老兄，再來一批，這時候CPU放下手邊的飲料再給GPU一批，但這中間不知不覺中浪費不少時間，如果可以跟豐田式管理一樣，都有一個預備批在那邊等，GPU拿走一批的時候CPU立馬補上一批，那中間就可以節省不少時間"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = datasets.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 468 steps\n",
      "Epoch 1/5\n",
      "468/468 [==============================] - 4s 10ms/step - loss: 0.9941 - accuracy: 0.7646\n",
      "Epoch 2/5\n",
      "468/468 [==============================] - 2s 5ms/step - loss: 0.3315 - accuracy: 0.9108\n",
      "Epoch 3/5\n",
      "468/468 [==============================] - 2s 5ms/step - loss: 0.2368 - accuracy: 0.9342\n",
      "Epoch 4/5\n",
      "468/468 [==============================] - 2s 5ms/step - loss: 0.1856 - accuracy: 0.9475\n",
      "Epoch 5/5\n",
      "468/468 [==============================] - 2s 5ms/step - loss: 0.1523 - accuracy: 0.9566\n",
      "CPU times: user 17.9 s, sys: 2.26 s, total: 20.1 s\n",
      "Wall time: 13.1 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f69c8273e50>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model.fit(datasets,\n",
    "          epochs=5, \n",
    "          steps_per_epoch=int(len(x_train)/128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面結果可以看的出來，即使差異不大，但prefetch的時間還是快了一咪咪，`buffer_size=tf.data.experimental.AUTOTUNE`即讓框架自己決定要預取多少資料，更多的資訊可參考官方[Optimize pipeline performance](https://www.tensorflow.org/guide/data_performance)說明"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "為了能夠說明，我們重新取資料，這次取資料並不先做資料標準化，只是單純的先多推一個軸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train.astype(dtype='float32')\n",
    "x_test = x_test.astype(dtype='float32')\n",
    "x_train = np.expand_dims(x_train, -1)\n",
    "x_test = np.expand_dims(x_test, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 28, 28, 1), (10000, 28, 28, 1))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35.108418"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tf.data.Dataset.from_tensor_slices`，由函數名稱不難看出，是一種取得資料的函數，來源需要是tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面範例意味著將我們取得的MNIST資料餵給`tf.data.Dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = tf.data.Dataset.from_tensor_slices((x_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35.108418\n"
     ]
    }
   ],
   "source": [
    "for _x in  datasets.take(1):\n",
    "    print(_x[0].numpy().mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接著我們希望可以將資料標準化或做某些前置預處理，也許原本的放入Dataset的資料是檔案路徑，我們需要利用路徑來載入照片轉為numpy再轉為tensor，因此我們需要設置一個函數"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "這個函數很簡單，只是為了做標準化而以，當然實際上不會這麼做，是為了範例才這麼做"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare(x_train_s, y_label_s):\n",
    "    x_train_s = x_train_s / 255.\n",
    "    return x_train_s, y_label_s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下來我們就可以利用`tf.data.Dataset.map`來綁定標準化函數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = datasets.map(map_func=prepare, \n",
    "                        num_parallel_calls=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13768007\n"
     ]
    }
   ],
   "source": [
    "for _x in  datasets.take(1):\n",
    "    print(_x[0].numpy().mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以發現到，我們的資料集已經標準化完成了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "訓練過程中的資料一定要打散，這可以透過`tf.data.Dataset.shuffle`來實作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = datasets.shuffle(buffer_size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16185975\n"
     ]
    }
   ],
   "source": [
    "for _x in  datasets.take(1):\n",
    "    print(_x[0].numpy().mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "資料已經亂掉了，要注意到的是，buffer_size設置愈小就愈不亂，代表設置1就是沒有洗牌，設置整個資料集數量就是整個大洗牌。舉例來說，buffer_size為10000，代表會從資料集拿10000筆資料先放到一個緩衝區去，然後再把這10000筆資料亂塞回去這原本的10000坑洞，因此如果設置1就是什麼都沒有交換了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最後，我們希望定義每一個batch的大小，避免訓練過程中是整個資料集塞進去GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = datasets.batch(128, drop_remainder=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "這邊參數`drop_remainder`指的是，當資料集大小/batch_size無法整除的時候是否放棄那不成批的資料"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最後不要忘了設置prefetch來增加效能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = datasets.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "現在就可以開始訓練了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 468 steps\n",
      "Epoch 1/5\n",
      "468/468 [==============================] - 2s 4ms/step - loss: 0.1279 - accuracy: 0.9631\n",
      "Epoch 2/5\n",
      "  1/468 [..............................] - ETA: 3s - loss: 0.0604 - accuracy: 1.0000WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 2340 batches). You may need to use the repeat() function when building your dataset.\n",
      "CPU times: user 7.03 s, sys: 2.03 s, total: 9.06 s\n",
      "Wall time: 2.1 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f6979439f10>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model.fit(datasets,\n",
    "          epochs=5, \n",
    "          steps_per_epoch=int(len(x_train)/128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "怎麼會這樣?我明明設置5個epoch，結果一個半就停了?這是因為，我們的資料集就是只有那麼多，迭代完了自然就停止了，因此最後還要設置一個`tf.data.Dataset.repeat`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = datasets.repeat(count=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "設置`count=-1`就代表天長地久，不會停止"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 468 steps\n",
      "Epoch 1/5\n",
      "468/468 [==============================] - 2s 4ms/step - loss: 0.1106 - accuracy: 0.9677\n",
      "Epoch 2/5\n",
      "468/468 [==============================] - 2s 4ms/step - loss: 0.0975 - accuracy: 0.9717\n",
      "Epoch 3/5\n",
      "468/468 [==============================] - 2s 4ms/step - loss: 0.0874 - accuracy: 0.9742\n",
      "Epoch 4/5\n",
      "468/468 [==============================] - 2s 4ms/step - loss: 0.0789 - accuracy: 0.9770\n",
      "Epoch 5/5\n",
      "468/468 [==============================] - 2s 4ms/step - loss: 0.0724 - accuracy: 0.9789\n",
      "CPU times: user 34.3 s, sys: 9.64 s, total: 44 s\n",
      "Wall time: 8.96 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f697ecbe610>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model.fit(datasets,\n",
    "          epochs=5, \n",
    "          steps_per_epoch=int(len(x_train)/128))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
